from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, List

from flask import current_app
from sqlalchemy.orm import selectinload

from ...models import PipelineRun
from ...utils.exceptions import ResourceNotFound
from ...utils.storage_paths import (
    run_directory,
    vulnerability_report_directory,
)
from ...utils.time import isoformat, utc_now
from ..data_management.structured_data_manager import StructuredDataManager
from .answer_scoring import AnswerScoringService
from .pdf_question_orchestrator import PDFQuestionEvaluator, QuestionPrompt


class VulnerabilityReportService:
    """Run the original PDF through multiple LLMs and score baseline accuracy."""

    REQUIRED_STAGES = {"content_discovery"}

    def __init__(self) -> None:
        self.structured_manager = StructuredDataManager()
        self.scoring_service = AnswerScoringService()

    def generate(self, run_id: str) -> Dict[str, Any]:
        run = (
            PipelineRun.query.options(
                selectinload(PipelineRun.questions),
                selectinload(PipelineRun.stages),
            )
            .filter_by(id=run_id)
            .one_or_none()
        )
        if not run:
            raise ResourceNotFound(f"Pipeline run {run_id} not found")

        self._guard_required_stages(run)

        structured = self.structured_manager.load(run_id) or {}
        questions = self._build_question_prompts(run, structured)
        if not questions:
            raise ValueError("No questions available to generate a vulnerability report.")

        pdf_path = Path(run.original_pdf_path)
        if not pdf_path.exists():
            raise ValueError(f"Original PDF missing at {pdf_path}")

        prompts = current_app.config.get("LLM_REPORT_PROMPTS") or [
            "Answer the referenced assessment question using the attached PDF.",
        ]
        evaluator = PDFQuestionEvaluator(prompts=prompts)
        evaluation = evaluator.evaluate(str(pdf_path), questions)
        scored_questions = self._score_questions(evaluation["questions"])
        
        # Build summary with all providers (including failed ones)
        # Pass evaluation["providers"] to ensure all attempted providers are included
        summary = self._build_summary(scored_questions, attempted_providers=evaluation.get("providers", []))

        payload = {
            "run_id": run_id,
            "report_type": "vulnerability",
            "generated_at": isoformat(utc_now()),
            "questions": scored_questions,
            "summary": summary,
            "providers": evaluation["providers"],
        }

        directory = vulnerability_report_directory(run_id)
        artifact_path = directory / "vulnerability_report.json"
        artifact_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding="utf-8")

        relative_path = str(artifact_path.resolve().relative_to(run_directory(run_id)))
        self._update_structured_data(run_id, payload, relative_path)

        return {
            **payload,
            "output_files": {"json": relative_path},
        }

    def _guard_required_stages(self, run: PipelineRun) -> None:
        stage_status = {stage.stage_name: stage.status for stage in run.stages}
        missing = [stage for stage in self.REQUIRED_STAGES if stage_status.get(stage) != "completed"]
        if missing:
            raise ValueError(
                f"Vulnerability report can only be generated after stages {', '.join(sorted(missing))} complete.",
            )

    def _build_question_prompts(self, run: PipelineRun, structured: Dict[str, Any]) -> List[QuestionPrompt]:
        structured_index = {
            str(entry.get("question_number") or entry.get("id")): entry for entry in (structured.get("questions") or [])
        }
        prompts: List[QuestionPrompt] = []
        for question in sorted(run.questions, key=lambda q: (q.sequence_index or 10**6, q.id)):
            number = str(question.question_number or question.id)
            structured_info = structured_index.get(number, {})
            prompts.append(
                QuestionPrompt(
                    question_id=question.id,
                    question_number=number,
                    question_text=structured_info.get("stem_text") or question.original_text,
                    question_type=question.question_type,
                    options=self._normalize_options(question.options_data, structured_info),
                    gold_answer=structured_info.get("gold_answer") or question.gold_answer,
                )
            )
        return prompts

    def _normalize_options(self, options_data: Any, structured_info: Dict[str, Any]) -> List[Dict[str, str]]:
        data = options_data or structured_info.get("options")
        normalized: List[Dict[str, str]] = []
        if isinstance(data, dict):
            for key, value in data.items():
                normalized.append({"label": str(key), "text": str(value)})
        elif isinstance(data, list):
            for entry in data:
                if isinstance(entry, dict):
                    label = entry.get("label") or entry.get("option") or entry.get("id")
                    text = entry.get("text") or entry.get("value") or entry.get("content")
                    normalized.append(
                        {
                            "label": str(label or len(normalized) + 1),
                            "text": str(text or ""),
                        }
                    )
                else:
                    normalized.append({"label": str(len(normalized) + 1), "text": str(entry)})
        return normalized

    def _score_questions(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        from ...utils.logging import get_logger
        logger = get_logger(__name__)
        
        scored: List[Dict[str, Any]] = []
        for entry in questions:
            answers = entry.get("answers", [])
            if not answers:
                logger.warning(
                    "Question %s has no answers from providers",
                    entry.get("question_number")
                )
                scored.append({**entry, "answers": []})
                continue
            
            try:
                batch_scores = self.scoring_service.score_batch(
                    question_text=entry.get("question_text", ""),
                    question_type=entry.get("question_type"),
                    gold_answer=entry.get("gold_answer"),
                    provider_answers=answers,
                    options=entry.get("options"),
                )
                score_lookup = {
                    score.get("provider"): score for score in batch_scores if score.get("provider")
                }
            except Exception as exc:
                logger.warning(
                    "Failed to score batch for question %s: %s. Scoring individually.",
                    entry.get("question_number"), exc
                )
                score_lookup = {}

            per_model = []
            for answer in answers:
                provider = answer.get("provider")
                scorecard = score_lookup.get(provider)
                if not scorecard:
                    try:
                        scorecard = self.scoring_service.score(
                            question_text=entry.get("question_text", ""),
                            question_type=entry.get("question_type"),
                            gold_answer=entry.get("gold_answer"),
                            candidate_answer=answer.get("answer_text") or answer.get("answer"),
                            options=entry.get("options"),
                        )
                    except Exception as exc:
                        logger.warning(
                            "Failed to score individual answer for provider %s, question %s: %s. Using fallback.",
                            provider, entry.get("question_number"), exc
                        )
                        scorecard = {
                            "score": 0.0,
                            "verdict": "missing",
                            "confidence": 0.0,
                            "rationale": f"Scoring failed: {str(exc)}",
                            "source": "heuristic",
                        }

                answer_error = answer.get("error")
                answer_source = "llm" if answer.get("success") else ("parse_error" if answer_error else "missing")

                per_model.append(
                    {
                        **answer,
                        "answer_source": answer_source,
                        "scorecard": scorecard,
                        "scoring_source": scorecard.get("source"),
                    }
                )
            scored.append({**entry, "answers": per_model})
        return scored

    def _build_summary(self, questions: List[Dict[str, Any]], attempted_providers: List[str] | None = None) -> Dict[str, Any]:
        totals: Dict[str, Dict[str, float]] = {}
        
        # Initialize all attempted providers to ensure they appear in summary even if they failed
        if attempted_providers:
            for provider in attempted_providers:
                totals.setdefault(provider, {"score_sum": 0.0, "count": 0})
        
        for entry in questions:
            for answer in entry.get("answers", []):
                provider = answer.get("provider") or "unknown"
                scorecard = answer.get("scorecard") or {}
                totals.setdefault(provider, {"score_sum": 0.0, "count": 0})
                totals[provider]["score_sum"] += float(scorecard.get("score", 0.0))
                totals[provider]["count"] += 1

        provider_summary = [
            {
                "provider": provider,
                "average_score": (values["score_sum"] / values["count"]) if values["count"] > 0 else 0.0,
                "questions_evaluated": int(values["count"]),
            }
            for provider, values in totals.items()
        ]
        provider_summary.sort(key=lambda item: item["provider"])

        return {
            "total_questions": len(questions),
            "providers": provider_summary,
        }

    def _update_structured_data(self, run_id: str, payload: Dict[str, Any], relative_path: str) -> None:
        structured = self.structured_manager.load(run_id) or {}
        reports = structured.setdefault("reports", {})
        reports["vulnerability"] = {
            "generated_at": payload["generated_at"],
            "summary": payload["summary"],
            "artifact": relative_path,
        }
        self.structured_manager.save(run_id, structured)
