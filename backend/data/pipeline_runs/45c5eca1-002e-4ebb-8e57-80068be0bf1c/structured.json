{
  "questions": [
    {
      "q_number": "1",
      "question_number": "1",
      "gold_answer": "C",
      "gold_confidence": 0.9,
      "question_type": "mcq_single",
      "original_text": "What is the primary benefit of stacking multiple RNN layers (i.e., stacked RNNs)?",
      "stem_text": "What is the primary benefit of stacking multiple RNN layers (i.e., stacked RNNs)?",
      "options": {
        "A": "Faster training",
        "B": "Lower memory usage",
        "C": "Better learning of hierarchical features",
        "D": "Simpler architecture"
      },
      "manipulation": {
        "method": "smart_substitution",
        "substring_mappings": [],
        "effectiveness_score": null,
        "character_strategy": "unicode_steganography"
      }
    },
    {
      "q_number": "2",
      "question_number": "2",
      "gold_answer": "B",
      "gold_confidence": 0.9,
      "question_type": "mcq_single",
      "original_text": "Which of the following is the main reason RNNs struggle with long-term dependencies?",
      "stem_text": "Which of the following is the main reason RNNs struggle with long-term dependencies?",
      "options": {
        "A": "Overfitting",
        "B": "Vanishing gradients",
        "C": "Lack of non-linearity",
        "D": "Insufficient data"
      },
      "manipulation": {
        "method": "smart_substitution",
        "substring_mappings": [],
        "effectiveness_score": null,
        "character_strategy": "unicode_steganography"
      }
    },
    {
      "q_number": "3",
      "question_number": "3",
      "gold_answer": "B",
      "gold_confidence": 0.9,
      "question_type": "mcq_single",
      "original_text": "What differentiates an LSTM cell from a standard RNN cell?",
      "stem_text": "What differentiates an LSTM cell from a standard RNN cell?",
      "options": {
        "A": "It uses ReLU instead of tanh",
        "B": "It introduces gates to control the flow of information",
        "C": "It has fewer parameters",
        "D": "It is a convolutional architecture"
      },
      "manipulation": {
        "method": "smart_substitution",
        "substring_mappings": [],
        "effectiveness_score": null,
        "character_strategy": "unicode_steganography"
      }
    },
    {
      "q_number": "4",
      "question_number": "4",
      "gold_answer": "B",
      "gold_confidence": 0.9,
      "question_type": "mcq_single",
      "original_text": "In a standard LSTM, which gate is responsible for deciding how much of the past memory to keep?",
      "stem_text": "In a standard LSTM, which gate is responsible for deciding how much of the past memory to keep?",
      "options": {
        "A": "Output gate",
        "B": "Forget gate",
        "C": "Input gate",
        "D": "Update gate"
      },
      "manipulation": {
        "method": "smart_substitution",
        "substring_mappings": [],
        "effectiveness_score": null,
        "character_strategy": "unicode_steganography"
      }
    },
    {
      "q_number": "5",
      "question_number": "5",
      "gold_answer": null,
      "gold_confidence": null,
      "question_type": "short_answer",
      "original_text": "Why is the forget gate bias in LSTMs often initialized to a high value (e.g., 2 or 3)? Explain its effect on long-term dependency learning.",
      "stem_text": "Why is the forget gate bias in LSTMs often initialized to a high value (e.g., 2 or 3)? Explain its effect on long-term dependency learning.",
      "manipulation": {
        "method": "smart_substitution",
        "substring_mappings": [],
        "effectiveness_score": null,
        "character_strategy": "unicode_steganography"
      }
    },
    {
      "q_number": "6",
      "question_number": "6",
      "gold_answer": null,
      "gold_confidence": null,
      "question_type": "short_answer",
      "original_text": "Bidirectional RNNs are often used for POS tagging but not machine translation. Explain why, considering input-output alignment and context flow.",
      "stem_text": "Bidirectional RNNs are often used for POS tagging but not machine translation. Explain why, considering input-output alignment and context flow.",
      "manipulation": {
        "method": "smart_substitution",
        "substring_mappings": [],
        "effectiveness_score": null,
        "character_strategy": "unicode_steganography"
      }
    },
    {
      "q_number": "7",
      "question_number": "7",
      "gold_answer": null,
      "gold_confidence": null,
      "question_type": "short_answer",
      "original_text": "Designing an RNN model for variable-length legal documents with long dependencies: (a) Choose between vanilla RNN or LSTM. (b) Stack layers or keep it shallow? (c) Make it bidirectional? Justify each choice based on model behavior and task needs.",
      "stem_text": "Designing an RNN model for variable-length legal documents with long dependencies: (a) Choose between vanilla RNN or LSTM. (b) Stack layers or keep it shallow? (c) Make it bidirectional? Justify each choice based on model behavior and task needs.",
      "manipulation": {
        "method": "smart_substitution",
        "substring_mappings": [],
        "effectiveness_score": null,
        "character_strategy": "unicode_steganography"
      }
    },
    {
      "q_number": "8",
      "question_number": "8",
      "gold_answer": null,
      "gold_confidence": null,
      "question_type": "short_answer",
      "original_text": "Consider a vanilla RNN with recurrent weight matrix W and sequence length 50. Analyze gradient behavior: (1) If ||W|| = 0.9: Will gradients vanish or explode? Justify. (2) If ||W|| = 1.2: Will gradients vanish or explode? Justify. Suggest an easy fix and explain how it helps.",
      "stem_text": "Consider a vanilla RNN with recurrent weight matrix W and sequence length 50. Analyze gradient behavior: (1) If ||W|| = 0.9: Will gradients vanish or explode? Justify. (2) If ||W|| = 1.2: Will gradients vanish or explode? Justify. Suggest an easy fix and explain how it helps.",
      "manipulation": {
        "method": "smart_substitution",
        "substring_mappings": [],
        "effectiveness_score": null,
        "character_strategy": "unicode_steganography"
      }
    }
  ],
  "global_mappings": {
    "character_strategy": "unicode_steganography",
    "mapping_dictionary": {
      "A": "А",
      "B": "В",
      "C": "С",
      "E": "Е",
      "H": "Н",
      "K": "К",
      "M": "М",
      "O": "О",
      "P": "Р",
      "T": "Т",
      "X": "Х",
      "Y": "Ү",
      "a": "а",
      "c": "с",
      "e": "е",
      "o": "о",
      "p": "р",
      "x": "х",
      "y": "у"
    },
    "effectiveness_stats": {
      "total_questions": 8,
      "average_effectiveness": 0.0,
      "total_characters_mapped": 19,
      "coverage_percentage": 20.0
    }
  },
  "pipeline_metadata": {
    "current_stage": "smart_substitution",
    "stages_completed": [
      "smart_substitution"
    ],
    "last_updated": "2025-09-28T20:40:05.050457+00:00"
  }
}